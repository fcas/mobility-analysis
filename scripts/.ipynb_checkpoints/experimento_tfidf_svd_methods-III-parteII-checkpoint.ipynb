{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data.csv', encoding='latin1', header=None)\n",
    "y = pd.read_csv('labels.csv', encoding='latin1', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.squeeze(data.values)\n",
    "y = np.squeeze(y.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing\n",
    "print(multiprocessing.cpu_count())\n",
    "seed = 42\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'''\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "tfidf=TfidfVectorizer(sublinear_tf=True, use_idf=True)\n",
    "X = tfidf.fit_transform(X)\n",
    "X.get_shape()\n",
    "svd = TruncatedSVD(n_components=3000, n_iter=7, random_state=42)\n",
    "svd.fit(X) \n",
    "print(svd.explained_variance_ratio_.sum())\n",
    "'''\n",
    "Inserir no artigo: \"Com o tf-idf tínhamos (59834, 13915), utilizamos o método SVD para redução de dimensionalidade e conseguimos antigir 90% da variância explicada pelos novos 3000 componentes gerados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Todos X_train e X_test em memória já transformados "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StratifiedKFold(n_splits=10, random_state=42, shuffle=True)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "skf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf=TfidfVectorizer(sublinear_tf=True, use_idf=True)\n",
    "svd = TruncatedSVD(n_components=3000, n_iter=7, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** ITERAÇÃO:  1\n",
      "antes do tfidf\n",
      "tf-idf:  (53849, 13272)\n",
      "tf-idf:  (5985, 13272)\n",
      "antes do svd\n",
      "svd:  (53849, 3000)\n",
      "svd:  (5985, 3000)\n",
      "*** ITERAÇÃO:  2\n",
      "antes do tfidf\n",
      "tf-idf:  (53849, 13227)\n",
      "tf-idf:  (5985, 13227)\n",
      "antes do svd\n",
      "svd:  (53849, 3000)\n",
      "svd:  (5985, 3000)\n",
      "*** ITERAÇÃO:  3\n",
      "antes do tfidf\n",
      "tf-idf:  (53849, 13252)\n",
      "tf-idf:  (5985, 13252)\n",
      "antes do svd\n",
      "svd:  (53849, 3000)\n",
      "svd:  (5985, 3000)\n",
      "*** ITERAÇÃO:  4\n",
      "antes do tfidf\n",
      "tf-idf:  (53850, 13279)\n",
      "tf-idf:  (5984, 13279)\n",
      "antes do svd\n",
      "svd:  (53850, 3000)\n",
      "svd:  (5984, 3000)\n",
      "*** ITERAÇÃO:  5\n",
      "antes do tfidf\n",
      "tf-idf:  (53850, 13306)\n",
      "tf-idf:  (5984, 13306)\n",
      "antes do svd\n",
      "svd:  (53850, 3000)\n",
      "svd:  (5984, 3000)\n",
      "*** ITERAÇÃO:  6\n",
      "antes do tfidf\n",
      "tf-idf:  (53850, 13244)\n",
      "tf-idf:  (5984, 13244)\n",
      "antes do svd\n",
      "svd:  (53850, 3000)\n",
      "svd:  (5984, 3000)\n",
      "*** ITERAÇÃO:  7\n",
      "antes do tfidf\n",
      "tf-idf:  (53851, 13252)\n",
      "tf-idf:  (5983, 13252)\n",
      "antes do svd\n",
      "svd:  (53851, 3000)\n",
      "svd:  (5983, 3000)\n",
      "*** ITERAÇÃO:  8\n",
      "antes do tfidf\n",
      "tf-idf:  (53851, 13224)\n",
      "tf-idf:  (5983, 13224)\n",
      "antes do svd\n",
      "svd:  (53851, 3000)\n",
      "svd:  (5983, 3000)\n",
      "*** ITERAÇÃO:  9\n",
      "antes do tfidf\n",
      "tf-idf:  (53853, 13305)\n",
      "tf-idf:  (5981, 13305)\n",
      "antes do svd\n",
      "svd:  (53853, 3000)\n",
      "svd:  (5981, 3000)\n",
      "*** ITERAÇÃO:  10\n",
      "antes do tfidf\n",
      "tf-idf:  (53854, 13265)\n",
      "tf-idf:  (5980, 13265)\n",
      "antes do svd\n",
      "svd:  (53854, 3000)\n",
      "svd:  (5980, 3000)\n"
     ]
    }
   ],
   "source": [
    "index = 1\n",
    "\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    print('*** ITERAÇÃO: ', index)\n",
    "    \n",
    "        \n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    print(\"antes do tfidf\")\n",
    "    X_train = tfidf.fit_transform(X_train)\n",
    "    X_test = tfidf.transform(X_test)\n",
    "    print('tf-idf: ', X_train.get_shape())\n",
    "    print('tf-idf: ', X_test.get_shape())\n",
    "\n",
    "    print(\"antes do svd\")\n",
    "    X_train = svd.fit_transform(X_train)\n",
    "    X_test = svd.transform(X_test)\n",
    "    print('svd: ', X_train.shape)\n",
    "    print('svd: ', X_test.shape)\n",
    "\n",
    "    \n",
    "    \n",
    "    X_train_ = \"X_train_\"\n",
    "    X_test_ = \"X_test_\"\n",
    "    y_train_ = \"y_train_\"\n",
    "    y_test_ = \"y_test_\"\n",
    "    globals()[X_train_ + str(index)] = X_train\n",
    "    globals()[X_test_ + str(index)] = X_test\n",
    "    globals()[y_train_ + str(index)] = y_train\n",
    "    globals()[y_test_ + str(index)] = y_test\n",
    "    \n",
    "    \n",
    "    index = index+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_data(iteracao):\n",
    "    print(\"arquivo a ser aberto: \", iteracao)\n",
    "    if iteracao == 1:\n",
    "        return X_train_1, X_test_1, y_train_1, y_test_1\n",
    "    elif iteracao == 2:\n",
    "        return X_train_2, X_test_2, y_train_2, y_test_2\n",
    "    elif iteracao == 3:\n",
    "        return X_train_3, X_test_3, y_train_3, y_test_3\n",
    "    elif iteracao == 4:\n",
    "        return X_train_4, X_test_4, y_train_4, y_test_4\n",
    "    elif iteracao == 5:\n",
    "        return X_train_5, X_test_5, y_train_5, y_test_5\n",
    "    elif iteracao == 6:\n",
    "        return X_train_6, X_test_6, y_train_6, y_test_6\n",
    "    elif iteracao == 7:\n",
    "        return X_train_7, X_test_7, y_train_7, y_test_7\n",
    "    elif iteracao == 8:\n",
    "        return X_train_8, X_test_8, y_train_8, y_test_8\n",
    "    elif iteracao == 9:\n",
    "        return X_train_9, X_test_9, y_train_9, y_test_9\n",
    "    elif iteracao == 10:\n",
    "        return X_train_10, X_test_10, y_train_10, y_test_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arquivo a ser aberto:  8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[ 2.05031321e-04,  8.09196694e-04, -5.77659118e-04, ...,\n",
       "         -9.97472242e-05,  4.40689220e-05, -1.36981206e-05],\n",
       "        [ 2.05031321e-04,  8.09196694e-04, -5.77659118e-04, ...,\n",
       "         -9.97472242e-05,  4.40689220e-05, -1.36981206e-05],\n",
       "        [ 1.07200209e-01,  1.18395748e-02, -1.04293248e-02, ...,\n",
       "          2.12746209e-04, -5.98536803e-04,  5.99941091e-04],\n",
       "        ...,\n",
       "        [ 6.06094525e-02,  2.96014329e-03, -4.05414838e-03, ...,\n",
       "          6.49612180e-04, -6.79078708e-03,  5.26551810e-03],\n",
       "        [ 3.09555157e-03,  6.30295151e-03, -5.20002890e-03, ...,\n",
       "         -2.44130402e-05, -5.00993419e-03,  6.16514337e-03],\n",
       "        [ 7.60256534e-02,  1.04358935e-02, -6.73748012e-03, ...,\n",
       "          2.85138110e-04, -3.36796572e-03,  4.50383059e-03]]),\n",
       " array([[ 2.05031321e-04,  8.09196694e-04, -5.77659118e-04, ...,\n",
       "         -1.17408722e-04,  3.92206166e-05,  5.23065504e-06],\n",
       "        [ 2.05031321e-04,  8.09196694e-04, -5.77659118e-04, ...,\n",
       "         -1.17408722e-04,  3.92206166e-05,  5.23065504e-06],\n",
       "        [ 4.60370088e-02,  2.06587673e-02, -1.53039699e-02, ...,\n",
       "          4.67607656e-04,  1.21465169e-04, -3.96129814e-03],\n",
       "        ...,\n",
       "        [ 6.46776267e-02,  5.18062950e-03, -4.35229976e-03, ...,\n",
       "          1.17019062e-02, -1.09082813e-03,  5.38012151e-03],\n",
       "        [ 8.73618756e-02,  1.11695647e-02, -7.56183381e-03, ...,\n",
       "          7.89214053e-04, -4.79833122e-03,  5.00926127e-03],\n",
       "        [ 3.35664779e-03,  1.95489572e-02, -1.85770480e-02, ...,\n",
       "          1.31194558e-03,  1.20658759e-02,  7.28317781e-03]]),\n",
       " array([0, 0, 0, ..., 0, 0, 0], dtype=int64),\n",
       " array([0, 0, 0, ..., 0, 0, 0], dtype=int64))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "return_data(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix, make_scorer, confusion_matrix, classification_report, roc_auc_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import tree, svm, neighbors, model_selection\n",
    "#from sklearn.naive_bayes import MultinomialNB, ComplementNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ParameterGrid\n",
    "models = []\n",
    "'''\n",
    "random_forest_clf = RandomForestClassifier()\n",
    "random_forest_clf.random_state = seed\n",
    "parameters = {\n",
    "    \"n_estimators\":[10, 20, 30],\n",
    "    \"max_depth\": [None, 5, 10, 15, 20]\n",
    "    \n",
    "}\n",
    "models.append((\"RandomForestClassifier\", random_forest_clf, list(ParameterGrid(parameters))))\n",
    "\n",
    "\n",
    "\n",
    "decision_tree_clf = tree.DecisionTreeClassifier()\n",
    "decision_tree_clf.random_state = seed\n",
    "parameters = {\n",
    "    \"max_depth\": [None, 2, 5, 8, 10, 15]\n",
    "}\n",
    "models.append((\"decision tree\", decision_tree_clf, list(ParameterGrid(parameters))))\n",
    "\n",
    "\n",
    "mnb_clf = MultinomialNB()\n",
    "parameters = {\n",
    "    \"alpha\": [0, 1, 3, 5, 10],\n",
    "    \"fit_prior\": [True]\n",
    "}\n",
    "mnb_clf.random_state = seed\n",
    "models.append((\"MultinomialNB\", mnb_clf, list(ParameterGrid(parameters))))\n",
    "\n",
    "\n",
    "knn_clf = neighbors.KNeighborsClassifier()\n",
    "knn_clf.random_state = seed\n",
    "parameters = {\n",
    "    \"n_neighbors\": [1, 3, 5, 7, 9],\n",
    "    \"metric\":[ \"cosine\", \"euclidean\"]\n",
    "}\n",
    "models.append((\"knn\", knn_clf, list(ParameterGrid(parameters))))\n",
    "\n",
    "\n",
    "cnb_clf = ComplementNB()\n",
    "parameters = {\n",
    "    \"alpha\": [0, 1, 3, 5],\n",
    "    \"norm\": [True, False]    \n",
    "}\n",
    "cnb_clf.random_state = seed\n",
    "models.append((\"ComplementNB\", cnb_clf, list(ParameterGrid(parameters))))\n",
    "'''\n",
    "\n",
    "logistic_regression_clf = LogisticRegression()\n",
    "logistic_regression_clf.random_state = seed\n",
    "parameters = {\n",
    "    \"penalty\": ['l1', 'l2'],\n",
    "    \"C\": [0.01, 0.1, 1, 5, 10, 15, 20]\n",
    "}\n",
    "models.append((\"logistic regression\", logistic_regression_clf, list(ParameterGrid(parameters))))\n",
    "\n",
    "\n",
    "svm_clf = svm.SVC()\n",
    "svm_clf.random_state = seed\n",
    "parameters = {\n",
    "    \"kernel\":('linear', 'rbf'), \n",
    "    \"C\":[0.1, 1, 5, 10, 15, 20]\n",
    "}\n",
    "models.append((\"svm\", svm_clf, list(ParameterGrid(parameters))))\n",
    "\n",
    "\n",
    "\n",
    "mlp_clf = MLPClassifier()\n",
    "mlp_clf.random_state = seed\n",
    "parameters = {\n",
    "    \"solver\": ['lbfgs'], \n",
    "    \"activation\":['relu'], \n",
    "    \"max_iter\": [2000], \n",
    "    \"alpha\": 10.0 ** -np.arange(1, 10), \n",
    "    \"tol\":[1e-3],\n",
    "    \"hidden_layer_sizes\":np.arange(10, 15)          \n",
    "}\n",
    "\n",
    "models.append((\"mlp\", mlp_clf, list(ParameterGrid(parameters))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modelo:  logistic regression\n",
      "[{'C': 0.01, 'penalty': 'l1'}, {'C': 0.01, 'penalty': 'l2'}, {'C': 0.1, 'penalty': 'l1'}, {'C': 0.1, 'penalty': 'l2'}, {'C': 1, 'penalty': 'l1'}, {'C': 1, 'penalty': 'l2'}, {'C': 5, 'penalty': 'l1'}, {'C': 5, 'penalty': 'l2'}, {'C': 10, 'penalty': 'l1'}, {'C': 10, 'penalty': 'l2'}, {'C': 15, 'penalty': 'l1'}, {'C': 15, 'penalty': 'l2'}, {'C': 20, 'penalty': 'l1'}, {'C': 20, 'penalty': 'l2'}]\n",
      "*** Parametro:  {'C': 0.01, 'penalty': 'l1'}\n",
      "1\n",
      "open files\n",
      "Thu Apr 25 22:50:24 2019\n",
      "arquivo a ser aberto:  1\n",
      "(53849, 3000)\n",
      "(5985, 3000)\n",
      "Thu Apr 25 22:50:24 2019\n",
      "fit model\n",
      "Thu Apr 25 22:51:57 2019\n",
      "predict\n",
      "Thu Apr 25 22:52:17 2019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "classification_report() got an unexpected keyword argument 'output_dict'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-c461f56ecb7c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     94\u001b[0m             \u001b[0mf1_micro_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf1_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'micro'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m             \u001b[0mf1_micro_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf1_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'micro'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 96\u001b[1;33m             \u001b[0mclassification_report_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     97\u001b[0m             \u001b[0mclassification_report_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m             \u001b[0mconf_matrix_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: classification_report() got an unexpected keyword argument 'output_dict'"
     ]
    }
   ],
   "source": [
    "results = pd.DataFrame()\n",
    "normalizer = MinMaxScaler(copy=False)\n",
    "\n",
    "for name, model, parameters in models:\n",
    "    print('modelo: ', name)\n",
    "    print(parameters)\n",
    "    \n",
    "    for parameter in parameters:\n",
    "        print('*** Parametro: ', parameter)\n",
    "        \n",
    "        #set parameter in the estimator\n",
    "        model.set_params(**parameter)\n",
    "        \n",
    "\n",
    "        #lista de metricas utilizadas para cada configuração experimental\n",
    "        accuracy_train = []\n",
    "        accuracy_test = []\n",
    "        precision_macro_train = []\n",
    "        precision_macro_test = []\n",
    "        precision_micro_train = []\n",
    "        precision_micro_test = []\n",
    "        recall_macro_train = []\n",
    "        recall_macro_test = []\n",
    "        recall_micro_train = []\n",
    "        recall_micro_test = []\n",
    "        f1_macro_train = []\n",
    "        f1_macro_test = []\n",
    "        f1_micro_train = []\n",
    "        f1_micro_test = []\n",
    "        classification_report_train = []\n",
    "        classification_report_test = []\n",
    "        conf_matrix_train = []\n",
    "        conf_matrix_test = []        \n",
    "        \n",
    "        #index para abrir os arquivos .csv que estão salvos pós tfidf e svd\n",
    "        for index in [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]:\n",
    "            print(index)\n",
    "            \n",
    "            #open file\n",
    "            print('open files')\n",
    "            import time\n",
    "            time = time.asctime()\n",
    "            print(time)\n",
    "            \n",
    "            X_train, X_test, y_train, y_test = return_data(index)\n",
    "            \n",
    "            print(X_train.shape)\n",
    "            print(X_test.shape)\n",
    "            \n",
    "            \n",
    "            #Normalização obrigatoria apenas para metodos que nao aceitam valores negativos (MinMaxScaler padronização)\n",
    "            if name == 'MultinomialNB' or name=='ComplementNB':\n",
    "                print(\"antes do normalizer\")\n",
    "                X_train = normalizer.fit_transform(X_train)\n",
    "                X_test = normalizer.transform(X_test)\n",
    "                print('normalizer: ', X_train.shape)\n",
    "                print('normalizer: ', X_test.shape)\n",
    "\n",
    "\n",
    "\n",
    "            #fit\n",
    "            import time\n",
    "            time = time.asctime()\n",
    "            print(time)\n",
    "            print('fit model')\n",
    "            model.fit(X_train, y_train)\n",
    "            \n",
    "            \n",
    "            import time\n",
    "            time = time.asctime()\n",
    "            print(time)\n",
    "            print('predict')\n",
    "            #predict train\n",
    "            y_pred_train = model.predict(X_train)\n",
    "            #predict test\n",
    "            y_pred_test = model.predict(X_test)\n",
    "            import time\n",
    "            time = time.asctime()\n",
    "            print(time)\n",
    "            \n",
    "            #append nas listas para cada métrica(y_true, y_pred)\n",
    "            accuracy_train.append(accuracy_score(y_train, y_pred_train))\n",
    "            accuracy_test.append(accuracy_score(y_test, y_pred_test))\n",
    "            precision_macro_train.append(precision_score(y_train, y_pred_train, average='macro'))\n",
    "            precision_macro_test.append(precision_score(y_test, y_pred_test, average='macro'))\n",
    "            precision_micro_train.append(precision_score(y_train, y_pred_train, average='micro'))\n",
    "            precision_micro_test.append(precision_score(y_test, y_pred_test, average='micro'))\n",
    "            recall_macro_train.append(recall_score(y_train, y_pred_train, average='macro'))\n",
    "            recall_macro_test.append(recall_score(y_test, y_pred_test, average='macro'))\n",
    "            recall_micro_train.append(recall_score(y_train, y_pred_train, average='micro'))\n",
    "            recall_micro_test.append(recall_score(y_test, y_pred_test, average='micro'))\n",
    "            f1_macro_train.append(f1_score(y_train, y_pred_train, average='macro'))\n",
    "            f1_macro_test.append(f1_score(y_test, y_pred_test, average='macro'))\n",
    "            f1_micro_train.append(f1_score(y_train, y_pred_train, average='micro'))\n",
    "            f1_micro_test.append(f1_score(y_test, y_pred_test, average='micro'))\n",
    "            classification_report_train.append(classification_report(y_train, y_pred_train, output_dict=True))\n",
    "            classification_report_test.append(classification_report(y_test, y_pred_test, output_dict=True))\n",
    "            conf_matrix_train.append(confusion_matrix(y_train, y_pred_train))\n",
    "            conf_matrix_test.append(confusion_matrix(y_test, y_pred_test))\n",
    "\n",
    "\n",
    "        #etapa para salvar os valores médios das listas, além das listas\n",
    "        d={\n",
    "            'model':[str(name)],\n",
    "            'parameter' : [str(parameter)],\n",
    "\n",
    "            'acc_train' : [str(accuracy_train)],\n",
    "\n",
    "            'acc_train_mean' : np.mean(accuracy_train),\n",
    "\n",
    "            'acc_test' : str(accuracy_test),\n",
    "            'acc_test_mean' : np.mean(accuracy_test),\n",
    "\n",
    "            'precision_macro_train' : str(precision_macro_train),\n",
    "            'precision_macro_train_mean' : np.mean(precision_macro_train),\n",
    "\n",
    "            'precision_macro_test' : str(precision_macro_test),\n",
    "            'precision_macro_test_mean' : np.mean(precision_macro_test),\n",
    "\n",
    "            'precision_micro_train' : str(precision_micro_train),\n",
    "            'precision_micro_train_mean' : np.mean(precision_micro_train),\n",
    "\n",
    "            'precision_micro_test' : str(precision_micro_test),\n",
    "            'precision_micro_test_mean' : np.mean(precision_micro_test),\n",
    "\n",
    "            'recall_macro_train' : str(recall_macro_train),\n",
    "            'recall_macro_train_mean' : np.mean(recall_macro_train),\n",
    "\n",
    "            'recall_macro_test' : str(recall_macro_test),\n",
    "            'recall_macro_test_mean' : np.mean(recall_macro_test),\n",
    "\n",
    "            'recall_micro_train' : str(recall_micro_train),\n",
    "            'recall_micro_train_mean' : np.mean(recall_micro_train),\n",
    "\n",
    "            'recall_micro_test' : str(recall_micro_test),\n",
    "            'recall_micro_test_mean' : np.mean(recall_micro_test),\n",
    "\n",
    "            'f1_macro_train' : str(f1_macro_train),\n",
    "            'f1_macro_train_mean' : np.mean(f1_macro_train),\n",
    "\n",
    "            'f1_macro_test' : str(f1_macro_test),\n",
    "            'f1_macro_test_mean' : np.mean(f1_macro_test),\n",
    "\n",
    "            'f1_micro_train' : str(f1_micro_train),\n",
    "            'f1_micro_train_mean' : np.mean(f1_micro_train),\n",
    "\n",
    "            'f1_micro_test' : str(f1_micro_test),\n",
    "            'f1_micro_test_mean' : np.mean(f1_micro_test),\n",
    "\n",
    "            'classification_report_train' : str(classification_report_train),\n",
    "            'classification_report_test' : str(classification_report_test),\n",
    "\n",
    "            'conf_matrix_train' : str(conf_matrix_train),\n",
    "            'conf_matrix_test' : str(conf_matrix_test)\n",
    "        }\n",
    "        \n",
    "        #gravo cada configuração experimental (metodo + parametros)\n",
    "        df = pd.DataFrame(d)\n",
    "        results = results.append(df, ignore_index=True)\n",
    "        \n",
    "        #sobrescrevendo o csv salvo para sempre manter o mais atualizado... se der pau, instancio do momento do erro\n",
    "        results.to_csv('resultados-tfidf-svd-models-versaoIII_parteII.csv', sep=';', decimal=',', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
