{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data.csv', encoding='latin1', header=None)\n",
    "y = pd.read_csv('labels.csv', encoding='latin1', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.squeeze(data.values)\n",
    "y = np.squeeze(y.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix, make_scorer\n",
    "from sklearn import tree, svm, neighbors, model_selection\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nao rodei experimentos para sklearn.naive_bayes.ComplementNB - est√° falhando o import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#from sklearn.naive_bayes import ComplementNB\\ncnb_clf = ComplementNB()\\nparameters = {\\n    \"complementnb__alpha\": [0, 1, 3, 5],\\n    \"complementnb__norm\": [True, False],\\n    \\n}\\ncnb_clf.random_state = seed\\nmodels.append((\"naive_bayes.ComplementNB\", cnb_clf, parameters))\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "#from sklearn.naive_bayes import ComplementNB\n",
    "cnb_clf = ComplementNB()\n",
    "parameters = {\n",
    "    \"complementnb__alpha\": [0, 1, 3, 5],\n",
    "    \"complementnb__norm\": [True, False],\n",
    "    \n",
    "}\n",
    "cnb_clf.random_state = seed\n",
    "models.append((\"naive_bayes.ComplementNB\", cnb_clf, parameters))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "\n",
    "'''\n",
    "random_forest_clf = RandomForestClassifier()\n",
    "random_forest_clf.random_state = seed\n",
    "parameters = {\n",
    "    \"randomforestclassifier__n_estimators\":[10, 20, 30],\n",
    "    \"randomforestclassifier__max_depth\": [None, 5, 10, 15, 20]\n",
    "}\n",
    "models.append((\"RandomForestClassifier\", random_forest_clf, parameters))\n",
    "\n",
    "\n",
    "\n",
    "mnb_clf = MultinomialNB()\n",
    "parameters = {\n",
    "    \"multinomialnb__alpha\": [0, 1, 3, 5, 10]\n",
    "}\n",
    "mnb_clf.random_state = seed\n",
    "models.append((\"naive_bayes.MultinomialNB\", mnb_clf, parameters))\n",
    "\n",
    "\n",
    "\n",
    "decision_tree_clf = tree.DecisionTreeClassifier()\n",
    "decision_tree_clf.random_state = seed\n",
    "parameters = {\n",
    "    \"decisiontreeclassifier__max_depth\": [None, 2, 5, 8, 10, 15]\n",
    "}\n",
    "models.append((\"decision tree\", decision_tree_clf, parameters))\n",
    "\n",
    "\n",
    "logistic_regression_clf = LogisticRegression()\n",
    "logistic_regression_clf.random_state = seed\n",
    "parameters = {\n",
    "    \"logisticregression__penalty\": ['l1', 'l2'],\n",
    "    \"logisticregression__C\": [0.01, 0.1, 1, 5, 10, 15, 20]\n",
    "}\n",
    "models.append((\"logistic regression\", logistic_regression_clf, parameters))\n",
    "'''\n",
    "\n",
    "svm_clf = svm.SVC()\n",
    "svm_clf.random_state = seed\n",
    "parameters = {'svc__kernel':('linear', 'rbf'), 'svc__C':[0.1, 1, 5, 10, 15, 20]}\n",
    "models.append((\"svm\", svm_clf, parameters))\n",
    "\n",
    "\n",
    "\n",
    "mlp_clf = MLPClassifier()\n",
    "mlp_clf.random_state = seed\n",
    "\n",
    "parameters = {'mlpclassifier__solver': ['lbfgs'], \n",
    "              'mlpclassifier__activation':['relu'], \n",
    "              'mlpclassifier__max_iter': [2000], \n",
    "              'mlpclassifier__alpha': 10.0 ** -np.arange(1, 10), \n",
    "              'mlpclassifier__tol':[1e-3], \n",
    "              'mlpclassifier__hidden_layer_sizes':np.arange(10, 15)}\n",
    "\n",
    "models.append((\"mlp\", mlp_clf, parameters))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "knn_clf = neighbors.KNeighborsClassifier()\n",
    "knn_clf.random_state = seed\n",
    "parameters = {\n",
    "    #\"kneighborsclassifier__n_neighbors\": [1, 3, 5, 7, 9],\n",
    "    \"kneighborsclassifier__n_neighbors\": [1],\n",
    "    \"kneighborsclassifier__metric\":[\"cosine\"]\n",
    "}\n",
    "models.append((\"knn\", knn_clf, parameters))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring = {'accuracy': make_scorer(accuracy_score),\n",
    "           'precision': make_scorer(precision_score, pos_label=None, average='micro'),\n",
    "           'recall': make_scorer(recall_score, pos_label=None, average='micro'),\n",
    "           'f1_score': make_scorer(f1_score, pos_label=None, average='micro')}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import fit_grid_point\n",
    "from sklearn.pipeline import make_pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "sklearn.cross_validation.StratifiedKFold(labels=[0 0 0 ... 0 0 0], n_folds=10, shuffle=True, random_state=42)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import cross_validation\n",
    "skf = cross_validation.StratifiedKFold(y, n_folds=10, shuffle=True, random_state=seed)\n",
    "skf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modelo:  svm\n",
      "{'svc__kernel': ('linear', 'rbf'), 'svc__C': [0.1, 1, 5, 10, 15, 20]}\n",
      "Fitting 10 folds for each of 12 candidates, totalling 120 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  10 tasks      | elapsed: 20.6min\n",
      "[Parallel(n_jobs=4)]: Done  64 tasks      | elapsed: 175.5min\n",
      "[Parallel(n_jobs=4)]: Done 120 out of 120 | elapsed: 336.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Estimator: \n",
      "Pipeline(memory=None,\n",
      "     steps=[('tfidfvectorizer', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_i...r',\n",
      "  max_iter=-1, probability=False, random_state=42, shrinking=True,\n",
      "  tol=0.001, verbose=False))])\n",
      "\n",
      "Best Parameters: \n",
      "{'svc__C': 5, 'svc__kernel': 'linear'}\n",
      "\n",
      "Best Test Score: \n",
      "0.978674332319417\n",
      "\n",
      "    mean_fit_time  mean_score_time  mean_test_accuracy  mean_test_f1_score  \\\n",
      "0       84.579734        32.612105            0.969298            0.969298   \n",
      "1      211.171917        79.185756            0.832436            0.832436   \n",
      "2       70.108810        22.651682            0.978474            0.978474   \n",
      "3      209.624833        78.945165            0.832436            0.832436   \n",
      "4       95.214962        21.155684            0.978674            0.978674   \n",
      "5      213.617483        79.021997            0.832436            0.832436   \n",
      "6       98.172923        19.580962            0.977270            0.977270   \n",
      "7      221.536229        80.173105            0.867550            0.867550   \n",
      "8      103.981340        19.266829            0.976385            0.976385   \n",
      "9      217.714880        78.134011            0.871461            0.871461   \n",
      "10     105.615303        19.115932            0.975165            0.975165   \n",
      "11     203.832255        73.317874            0.876926            0.876926   \n",
      "\n",
      "    mean_test_precision  mean_test_recall  mean_train_accuracy  \\\n",
      "0              0.969298          0.969298             0.971233   \n",
      "1              0.832436          0.832436             0.832436   \n",
      "2              0.978474          0.978474             0.986054   \n",
      "3              0.832436          0.832436             0.832436   \n",
      "4              0.978674          0.978674             0.992193   \n",
      "5              0.832436          0.832436             0.832436   \n",
      "6              0.977270          0.977270             0.993774   \n",
      "7              0.867550          0.867550             0.866728   \n",
      "8              0.976385          0.976385             0.994518   \n",
      "9              0.871461          0.871461             0.870900   \n",
      "10             0.975165          0.975165             0.994962   \n",
      "11             0.876926          0.876926             0.876219   \n",
      "\n",
      "    mean_train_f1_score  mean_train_precision  mean_train_recall  \\\n",
      "0              0.971233              0.971233           0.971233   \n",
      "1              0.832436              0.832436           0.832436   \n",
      "2              0.986054              0.986054           0.986054   \n",
      "3              0.832436              0.832436           0.832436   \n",
      "4              0.992193              0.992193           0.992193   \n",
      "5              0.832436              0.832436           0.832436   \n",
      "6              0.993774              0.993774           0.993774   \n",
      "7              0.866728              0.866728           0.866728   \n",
      "8              0.994518              0.994518           0.994518   \n",
      "9              0.870900              0.870900           0.870900   \n",
      "10             0.994962              0.994962           0.994962   \n",
      "11             0.876219              0.876219           0.876219   \n",
      "\n",
      "          ...        std_fit_time std_score_time std_test_accuracy  \\\n",
      "0         ...            1.426705       0.768733          0.001716   \n",
      "1         ...            2.607469       0.858699          0.000169   \n",
      "2         ...            3.278946       0.869434          0.002177   \n",
      "3         ...            3.336108       0.941648          0.000169   \n",
      "4         ...            7.516586       0.951090          0.001233   \n",
      "5         ...            3.448425       0.820498          0.000169   \n",
      "6         ...            8.946450       0.981369          0.001363   \n",
      "7         ...            2.643771       0.631014          0.001144   \n",
      "8         ...            7.307802       0.523823          0.001201   \n",
      "9         ...            2.374365       0.757069          0.001390   \n",
      "10        ...            5.544340       0.530187          0.001510   \n",
      "11        ...           16.730756       7.739978          0.001618   \n",
      "\n",
      "   std_test_f1_score  std_test_precision  std_test_recall  std_train_accuracy  \\\n",
      "0           0.001716            0.001716         0.001716            0.000176   \n",
      "1           0.000169            0.000169         0.000169            0.000019   \n",
      "2           0.002177            0.002177         0.002177            0.000171   \n",
      "3           0.000169            0.000169         0.000169            0.000019   \n",
      "4           0.001233            0.001233         0.001233            0.000133   \n",
      "5           0.000169            0.000169         0.000169            0.000019   \n",
      "6           0.001363            0.001363         0.001363            0.000181   \n",
      "7           0.001144            0.001144         0.001144            0.000148   \n",
      "8           0.001201            0.001201         0.001201            0.000209   \n",
      "9           0.001390            0.001390         0.001390            0.000452   \n",
      "10          0.001510            0.001510         0.001510            0.000200   \n",
      "11          0.001618            0.001618         0.001618            0.000225   \n",
      "\n",
      "    std_train_f1_score  std_train_precision  std_train_recall  \n",
      "0             0.000176             0.000176          0.000176  \n",
      "1             0.000019             0.000019          0.000019  \n",
      "2             0.000171             0.000171          0.000171  \n",
      "3             0.000019             0.000019          0.000019  \n",
      "4             0.000133             0.000133          0.000133  \n",
      "5             0.000019             0.000019          0.000019  \n",
      "6             0.000181             0.000181          0.000181  \n",
      "7             0.000148             0.000148          0.000148  \n",
      "8             0.000209             0.000209          0.000209  \n",
      "9             0.000452             0.000452          0.000452  \n",
      "10            0.000200             0.000200          0.000200  \n",
      "11            0.000225             0.000225          0.000225  \n",
      "\n",
      "[12 rows x 108 columns]\n",
      "_______________________________________________________\n",
      "modelo:  mlp\n",
      "{'mlpclassifier__solver': ['lbfgs'], 'mlpclassifier__activation': ['relu'], 'mlpclassifier__max_iter': [2000], 'mlpclassifier__alpha': array([1.e-01, 1.e-02, 1.e-03, 1.e-04, 1.e-05, 1.e-06, 1.e-07, 1.e-08,\n",
      "       1.e-09]), 'mlpclassifier__tol': [0.001], 'mlpclassifier__hidden_layer_sizes': array([10, 11, 12, 13, 14])}\n",
      "Fitting 10 folds for each of 45 candidates, totalling 450 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  10 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=4)]: Done  64 tasks      | elapsed: 12.3min\n",
      "[Parallel(n_jobs=4)]: Done 154 tasks      | elapsed: 31.9min\n",
      "[Parallel(n_jobs=4)]: Done 280 tasks      | elapsed: 55.2min\n",
      "[Parallel(n_jobs=4)]: Done 442 tasks      | elapsed: 84.7min\n",
      "[Parallel(n_jobs=4)]: Done 450 out of 450 | elapsed: 86.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Estimator: \n",
      "Pipeline(memory=None,\n",
      "     steps=[('tfidfvectorizer', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_i...       solver='lbfgs', tol=0.001, validation_fraction=0.1, verbose=False,\n",
      "       warm_start=False))])\n",
      "\n",
      "Best Parameters: \n",
      "{'mlpclassifier__activation': 'relu', 'mlpclassifier__alpha': 1e-06, 'mlpclassifier__hidden_layer_sizes': 12, 'mlpclassifier__max_iter': 2000, 'mlpclassifier__solver': 'lbfgs', 'mlpclassifier__tol': 0.001}\n",
      "\n",
      "Best Test Score: \n",
      "0.9763512384263128\n",
      "\n",
      "    mean_fit_time  mean_score_time  mean_test_accuracy  mean_test_f1_score  \\\n",
      "0       84.579734        32.612105            0.969298            0.969298   \n",
      "1      211.171917        79.185756            0.832436            0.832436   \n",
      "2       70.108810        22.651682            0.978474            0.978474   \n",
      "3      209.624833        78.945165            0.832436            0.832436   \n",
      "4       95.214962        21.155684            0.978674            0.978674   \n",
      "5      213.617483        79.021997            0.832436            0.832436   \n",
      "6       98.172923        19.580962            0.977270            0.977270   \n",
      "7      221.536229        80.173105            0.867550            0.867550   \n",
      "8      103.981340        19.266829            0.976385            0.976385   \n",
      "9      217.714880        78.134011            0.871461            0.871461   \n",
      "10     105.615303        19.115932            0.975165            0.975165   \n",
      "11     203.832255        73.317874            0.876926            0.876926   \n",
      "12      43.862396         0.967269            0.975566            0.975566   \n",
      "13      33.898435         0.924138            0.975449            0.975449   \n",
      "14      36.963244         0.941118            0.975766            0.975766   \n",
      "15      34.465173         0.845127            0.975816            0.975816   \n",
      "16      38.853619         0.899171            0.975950            0.975950   \n",
      "17      31.348830         0.862532            0.975516            0.975516   \n",
      "18      41.743242         0.851276            0.975365            0.975365   \n",
      "19      35.022036         0.900233            0.975733            0.975733   \n",
      "20      51.639780         0.908402            0.975532            0.975532   \n",
      "21      31.429196         0.896253            0.975633            0.975633   \n",
      "22      38.631549         0.824162            0.975048            0.975048   \n",
      "23      45.976997         0.922942            0.975131            0.975131   \n",
      "24      39.628516         0.932772            0.975750            0.975750   \n",
      "25      44.641799         1.075020            0.975883            0.975883   \n",
      "26      53.522142         1.172168            0.975599            0.975599   \n",
      "27      42.495559         1.039850            0.975415            0.975415   \n",
      "28      49.356889         1.048897            0.975114            0.975114   \n",
      "29      32.731818         0.985989            0.975984            0.975984   \n",
      "30      43.844549         0.869307            0.975599            0.975599   \n",
      "31      51.413862         1.000464            0.975883            0.975883   \n",
      "32      28.178586         0.835182            0.975282            0.975282   \n",
      "33      34.860236         0.856367            0.975298            0.975298   \n",
      "34      31.198251         0.947350            0.975716            0.975716   \n",
      "35      29.250837         0.971265            0.975582            0.975582   \n",
      "36      34.677045         0.851606            0.975616            0.975616   \n",
      "37      30.345878         0.837575            0.975415            0.975415   \n",
      "38      31.859149         0.832764            0.975114            0.975114   \n",
      "39      30.071766         0.828679            0.976351            0.976351   \n",
      "40      36.210368         0.856293            0.975215            0.975215   \n",
      "41      35.411752         0.850645            0.976318            0.976318   \n",
      "42      28.200009         0.820488            0.975332            0.975332   \n",
      "43      31.658553         0.833804            0.975248            0.975248   \n",
      "44      36.994623         0.944649            0.976067            0.976067   \n",
      "45      39.481795         0.876894            0.976050            0.976050   \n",
      "46      46.709545         1.015417            0.975883            0.975883   \n",
      "47      33.223172         0.885485            0.975415            0.975415   \n",
      "48      32.254766         0.883192            0.975415            0.975415   \n",
      "49      33.340845         0.866200            0.976084            0.976084   \n",
      "50      42.399994         0.906415            0.975449            0.975449   \n",
      "51      41.795264         0.882907            0.975332            0.975332   \n",
      "52      25.191939         0.826136            0.975800            0.975800   \n",
      "53      30.603252         0.863631            0.974814            0.974814   \n",
      "54      28.148059         0.848827            0.976017            0.976017   \n",
      "55      34.834349         0.869320            0.975900            0.975900   \n",
      "56      37.377040         0.819372            0.975733            0.975733   \n",
      "\n",
      "    mean_test_precision  mean_test_recall  mean_train_accuracy  \\\n",
      "0              0.969298          0.969298             0.971233   \n",
      "1              0.832436          0.832436             0.832436   \n",
      "2              0.978474          0.978474             0.986054   \n",
      "3              0.832436          0.832436             0.832436   \n",
      "4              0.978674          0.978674             0.992193   \n",
      "5              0.832436          0.832436             0.832436   \n",
      "6              0.977270          0.977270             0.993774   \n",
      "7              0.867550          0.867550             0.866728   \n",
      "8              0.976385          0.976385             0.994518   \n",
      "9              0.871461          0.871461             0.870900   \n",
      "10             0.975165          0.975165             0.994962   \n",
      "11             0.876926          0.876926             0.876219   \n",
      "12             0.975566          0.975566             0.995163   \n",
      "13             0.975449          0.975449             0.995112   \n",
      "14             0.975766          0.975766             0.995411   \n",
      "15             0.975816          0.975816             0.994600   \n",
      "16             0.975950          0.975950             0.993712   \n",
      "17             0.975516          0.975516             0.993859   \n",
      "18             0.975365          0.975365             0.994917   \n",
      "19             0.975733          0.975733             0.994734   \n",
      "20             0.975532          0.975532             0.995231   \n",
      "21             0.975633          0.975633             0.994568   \n",
      "22             0.975048          0.975048             0.995592   \n",
      "23             0.975131          0.975131             0.994739   \n",
      "24             0.975750          0.975750             0.994834   \n",
      "25             0.975883          0.975883             0.993538   \n",
      "26             0.975599          0.975599             0.995898   \n",
      "27             0.975415          0.975415             0.994579   \n",
      "28             0.975114          0.975114             0.994418   \n",
      "29             0.975984          0.975984             0.993822   \n",
      "30             0.975599          0.975599             0.995077   \n",
      "31             0.975883          0.975883             0.994457   \n",
      "32             0.975282          0.975282             0.994572   \n",
      "33             0.975298          0.975298             0.994670   \n",
      "34             0.975716          0.975716             0.994631   \n",
      "35             0.975582          0.975582             0.994255   \n",
      "36             0.975616          0.975616             0.994683   \n",
      "37             0.975415          0.975415             0.994477   \n",
      "38             0.975114          0.975114             0.994513   \n",
      "39             0.976351          0.976351             0.993495   \n",
      "40             0.975215          0.975215             0.995400   \n",
      "41             0.976318          0.976318             0.993991   \n",
      "42             0.975332          0.975332             0.994956   \n",
      "43             0.975248          0.975248             0.994089   \n",
      "44             0.976067          0.976067             0.994836   \n",
      "45             0.976050          0.976050             0.995023   \n",
      "46             0.975883          0.975883             0.995044   \n",
      "47             0.975415          0.975415             0.994143   \n",
      "48             0.975415          0.975415             0.994002   \n",
      "49             0.976084          0.976084             0.994256   \n",
      "50             0.975449          0.975449             0.995556   \n",
      "51             0.975332          0.975332             0.995350   \n",
      "52             0.975800          0.975800             0.994242   \n",
      "53             0.974814          0.974814             0.994691   \n",
      "54             0.976017          0.976017             0.994451   \n",
      "55             0.975900          0.975900             0.995738   \n",
      "56             0.975733          0.975733             0.995280   \n",
      "\n",
      "    mean_train_f1_score  mean_train_precision  mean_train_recall  \\\n",
      "0              0.971233              0.971233           0.971233   \n",
      "1              0.832436              0.832436           0.832436   \n",
      "2              0.986054              0.986054           0.986054   \n",
      "3              0.832436              0.832436           0.832436   \n",
      "4              0.992193              0.992193           0.992193   \n",
      "5              0.832436              0.832436           0.832436   \n",
      "6              0.993774              0.993774           0.993774   \n",
      "7              0.866728              0.866728           0.866728   \n",
      "8              0.994518              0.994518           0.994518   \n",
      "9              0.870900              0.870900           0.870900   \n",
      "10             0.994962              0.994962           0.994962   \n",
      "11             0.876219              0.876219           0.876219   \n",
      "12             0.995163              0.995163           0.995163   \n",
      "13             0.995112              0.995112           0.995112   \n",
      "14             0.995411              0.995411           0.995411   \n",
      "15             0.994600              0.994600           0.994600   \n",
      "16             0.993712              0.993712           0.993712   \n",
      "17             0.993859              0.993859           0.993859   \n",
      "18             0.994917              0.994917           0.994917   \n",
      "19             0.994734              0.994734           0.994734   \n",
      "20             0.995231              0.995231           0.995231   \n",
      "21             0.994568              0.994568           0.994568   \n",
      "22             0.995592              0.995592           0.995592   \n",
      "23             0.994739              0.994739           0.994739   \n",
      "24             0.994834              0.994834           0.994834   \n",
      "25             0.993538              0.993538           0.993538   \n",
      "26             0.995898              0.995898           0.995898   \n",
      "27             0.994579              0.994579           0.994579   \n",
      "28             0.994418              0.994418           0.994418   \n",
      "29             0.993822              0.993822           0.993822   \n",
      "30             0.995077              0.995077           0.995077   \n",
      "31             0.994457              0.994457           0.994457   \n",
      "32             0.994572              0.994572           0.994572   \n",
      "33             0.994670              0.994670           0.994670   \n",
      "34             0.994631              0.994631           0.994631   \n",
      "35             0.994255              0.994255           0.994255   \n",
      "36             0.994683              0.994683           0.994683   \n",
      "37             0.994477              0.994477           0.994477   \n",
      "38             0.994513              0.994513           0.994513   \n",
      "39             0.993495              0.993495           0.993495   \n",
      "40             0.995400              0.995400           0.995400   \n",
      "41             0.993991              0.993991           0.993991   \n",
      "42             0.994956              0.994956           0.994956   \n",
      "43             0.994089              0.994089           0.994089   \n",
      "44             0.994836              0.994836           0.994836   \n",
      "45             0.995023              0.995023           0.995023   \n",
      "46             0.995044              0.995044           0.995044   \n",
      "47             0.994143              0.994143           0.994143   \n",
      "48             0.994002              0.994002           0.994002   \n",
      "49             0.994256              0.994256           0.994256   \n",
      "50             0.995556              0.995556           0.995556   \n",
      "51             0.995350              0.995350           0.995350   \n",
      "52             0.994242              0.994242           0.994242   \n",
      "53             0.994691              0.994691           0.994691   \n",
      "54             0.994451              0.994451           0.994451   \n",
      "55             0.995738              0.995738           0.995738   \n",
      "56             0.995280              0.995280           0.995280   \n",
      "\n",
      "         ...        std_fit_time std_score_time std_test_accuracy  \\\n",
      "0        ...            1.426705       0.768733          0.001716   \n",
      "1        ...            2.607469       0.858699          0.000169   \n",
      "2        ...            3.278946       0.869434          0.002177   \n",
      "3        ...            3.336108       0.941648          0.000169   \n",
      "4        ...            7.516586       0.951090          0.001233   \n",
      "5        ...            3.448425       0.820498          0.000169   \n",
      "6        ...            8.946450       0.981369          0.001363   \n",
      "7        ...            2.643771       0.631014          0.001144   \n",
      "8        ...            7.307802       0.523823          0.001201   \n",
      "9        ...            2.374365       0.757069          0.001390   \n",
      "10       ...            5.544340       0.530187          0.001510   \n",
      "11       ...           16.730756       7.739978          0.001618   \n",
      "12       ...           15.066185       0.268302          0.001774   \n",
      "13       ...            9.904854       0.292678          0.001745   \n",
      "14       ...           17.378186       0.196625          0.001991   \n",
      "15       ...           12.144875       0.124553          0.002018   \n",
      "16       ...           18.783274       0.116875          0.001783   \n",
      "17       ...           16.029411       0.116904          0.001579   \n",
      "18       ...           16.662795       0.067981          0.001887   \n",
      "19       ...           14.680462       0.072679          0.002154   \n",
      "20       ...           36.799312       0.120425          0.001584   \n",
      "21       ...            9.789145       0.149592          0.001645   \n",
      "22       ...           17.224583       0.075862          0.001318   \n",
      "23       ...           19.176248       0.086986          0.001651   \n",
      "24       ...           12.046279       0.187399          0.001685   \n",
      "25       ...           21.433237       0.279800          0.001680   \n",
      "26       ...           21.109006       0.374184          0.001066   \n",
      "27       ...           13.624940       0.280833          0.001680   \n",
      "28       ...           32.344762       0.209590          0.001173   \n",
      "29       ...           13.103040       0.271678          0.001536   \n",
      "30       ...           11.028080       0.177256          0.001716   \n",
      "31       ...           10.607732       0.371603          0.001969   \n",
      "32       ...            9.023680       0.076067          0.001450   \n",
      "33       ...           12.330128       0.102045          0.001919   \n",
      "34       ...           12.027948       0.279363          0.001555   \n",
      "35       ...           16.600429       0.314660          0.002234   \n",
      "36       ...           12.586505       0.064521          0.001676   \n",
      "37       ...            9.500728       0.017447          0.001660   \n",
      "38       ...            8.732207       0.040973          0.001584   \n",
      "39       ...           11.129428       0.036869          0.001314   \n",
      "40       ...            5.619760       0.050295          0.001715   \n",
      "41       ...            8.713805       0.048271          0.001787   \n",
      "42       ...           10.829313       0.058227          0.001473   \n",
      "43       ...           10.440614       0.056513          0.001734   \n",
      "44       ...           16.701681       0.273053          0.002027   \n",
      "45       ...           16.626926       0.108625          0.001583   \n",
      "46       ...           21.950563       0.274383          0.001511   \n",
      "47       ...           14.585977       0.103568          0.001094   \n",
      "48       ...            9.097302       0.111901          0.001421   \n",
      "49       ...           12.524932       0.084291          0.001880   \n",
      "50       ...           17.581089       0.167216          0.001894   \n",
      "51       ...           12.181314       0.109566          0.001052   \n",
      "52       ...            4.852452       0.026066          0.001228   \n",
      "53       ...           10.247588       0.083453          0.000746   \n",
      "54       ...            8.149506       0.067207          0.001684   \n",
      "55       ...            9.887205       0.117577          0.001811   \n",
      "56       ...           18.531263       0.142296          0.001432   \n",
      "\n",
      "   std_test_f1_score std_test_precision std_test_recall std_train_accuracy  \\\n",
      "0           0.001716           0.001716        0.001716           0.000176   \n",
      "1           0.000169           0.000169        0.000169           0.000019   \n",
      "2           0.002177           0.002177        0.002177           0.000171   \n",
      "3           0.000169           0.000169        0.000169           0.000019   \n",
      "4           0.001233           0.001233        0.001233           0.000133   \n",
      "5           0.000169           0.000169        0.000169           0.000019   \n",
      "6           0.001363           0.001363        0.001363           0.000181   \n",
      "7           0.001144           0.001144        0.001144           0.000148   \n",
      "8           0.001201           0.001201        0.001201           0.000209   \n",
      "9           0.001390           0.001390        0.001390           0.000452   \n",
      "10          0.001510           0.001510        0.001510           0.000200   \n",
      "11          0.001618           0.001618        0.001618           0.000225   \n",
      "12          0.001774           0.001774        0.001774           0.001599   \n",
      "13          0.001745           0.001745        0.001745           0.001324   \n",
      "14          0.001991           0.001991        0.001991           0.000795   \n",
      "15          0.002018           0.002018        0.002018           0.001737   \n",
      "16          0.001783           0.001783        0.001783           0.003402   \n",
      "17          0.001579           0.001579        0.001579           0.001471   \n",
      "18          0.001887           0.001887        0.001887           0.001718   \n",
      "19          0.002154           0.002154        0.002154           0.001940   \n",
      "20          0.001584           0.001584        0.001584           0.001934   \n",
      "21          0.001645           0.001645        0.001645           0.001390   \n",
      "22          0.001318           0.001318        0.001318           0.000912   \n",
      "23          0.001651           0.001651        0.001651           0.000880   \n",
      "24          0.001685           0.001685        0.001685           0.001841   \n",
      "25          0.001680           0.001680        0.001680           0.002455   \n",
      "26          0.001066           0.001066        0.001066           0.001070   \n",
      "27          0.001680           0.001680        0.001680           0.001230   \n",
      "28          0.001173           0.001173        0.001173           0.001529   \n",
      "29          0.001536           0.001536        0.001536           0.001324   \n",
      "30          0.001716           0.001716        0.001716           0.001155   \n",
      "31          0.001969           0.001969        0.001969           0.002609   \n",
      "32          0.001450           0.001450        0.001450           0.001091   \n",
      "33          0.001919           0.001919        0.001919           0.001654   \n",
      "34          0.001555           0.001555        0.001555           0.001249   \n",
      "35          0.002234           0.002234        0.002234           0.002046   \n",
      "36          0.001676           0.001676        0.001676           0.002601   \n",
      "37          0.001660           0.001660        0.001660           0.001298   \n",
      "38          0.001584           0.001584        0.001584           0.001859   \n",
      "39          0.001314           0.001314        0.001314           0.002814   \n",
      "40          0.001715           0.001715        0.001715           0.001392   \n",
      "41          0.001787           0.001787        0.001787           0.001971   \n",
      "42          0.001473           0.001473        0.001473           0.001432   \n",
      "43          0.001734           0.001734        0.001734           0.002101   \n",
      "44          0.002027           0.002027        0.002027           0.001268   \n",
      "45          0.001583           0.001583        0.001583           0.001166   \n",
      "46          0.001511           0.001511        0.001511           0.001279   \n",
      "47          0.001094           0.001094        0.001094           0.001388   \n",
      "48          0.001421           0.001421        0.001421           0.002030   \n",
      "49          0.001880           0.001880        0.001880           0.001579   \n",
      "50          0.001894           0.001894        0.001894           0.001014   \n",
      "51          0.001052           0.001052        0.001052           0.001192   \n",
      "52          0.001228           0.001228        0.001228           0.001129   \n",
      "53          0.000746           0.000746        0.000746           0.001659   \n",
      "54          0.001684           0.001684        0.001684           0.001111   \n",
      "55          0.001811           0.001811        0.001811           0.001272   \n",
      "56          0.001432           0.001432        0.001432           0.001562   \n",
      "\n",
      "   std_train_f1_score std_train_precision std_train_recall  \n",
      "0            0.000176            0.000176         0.000176  \n",
      "1            0.000019            0.000019         0.000019  \n",
      "2            0.000171            0.000171         0.000171  \n",
      "3            0.000019            0.000019         0.000019  \n",
      "4            0.000133            0.000133         0.000133  \n",
      "5            0.000019            0.000019         0.000019  \n",
      "6            0.000181            0.000181         0.000181  \n",
      "7            0.000148            0.000148         0.000148  \n",
      "8            0.000209            0.000209         0.000209  \n",
      "9            0.000452            0.000452         0.000452  \n",
      "10           0.000200            0.000200         0.000200  \n",
      "11           0.000225            0.000225         0.000225  \n",
      "12           0.001599            0.001599         0.001599  \n",
      "13           0.001324            0.001324         0.001324  \n",
      "14           0.000795            0.000795         0.000795  \n",
      "15           0.001737            0.001737         0.001737  \n",
      "16           0.003402            0.003402         0.003402  \n",
      "17           0.001471            0.001471         0.001471  \n",
      "18           0.001718            0.001718         0.001718  \n",
      "19           0.001940            0.001940         0.001940  \n",
      "20           0.001934            0.001934         0.001934  \n",
      "21           0.001390            0.001390         0.001390  \n",
      "22           0.000912            0.000912         0.000912  \n",
      "23           0.000880            0.000880         0.000880  \n",
      "24           0.001841            0.001841         0.001841  \n",
      "25           0.002455            0.002455         0.002455  \n",
      "26           0.001070            0.001070         0.001070  \n",
      "27           0.001230            0.001230         0.001230  \n",
      "28           0.001529            0.001529         0.001529  \n",
      "29           0.001324            0.001324         0.001324  \n",
      "30           0.001155            0.001155         0.001155  \n",
      "31           0.002609            0.002609         0.002609  \n",
      "32           0.001091            0.001091         0.001091  \n",
      "33           0.001654            0.001654         0.001654  \n",
      "34           0.001249            0.001249         0.001249  \n",
      "35           0.002046            0.002046         0.002046  \n",
      "36           0.002601            0.002601         0.002601  \n",
      "37           0.001298            0.001298         0.001298  \n",
      "38           0.001859            0.001859         0.001859  \n",
      "39           0.002814            0.002814         0.002814  \n",
      "40           0.001392            0.001392         0.001392  \n",
      "41           0.001971            0.001971         0.001971  \n",
      "42           0.001432            0.001432         0.001432  \n",
      "43           0.002101            0.002101         0.002101  \n",
      "44           0.001268            0.001268         0.001268  \n",
      "45           0.001166            0.001166         0.001166  \n",
      "46           0.001279            0.001279         0.001279  \n",
      "47           0.001388            0.001388         0.001388  \n",
      "48           0.002030            0.002030         0.002030  \n",
      "49           0.001579            0.001579         0.001579  \n",
      "50           0.001014            0.001014         0.001014  \n",
      "51           0.001192            0.001192         0.001192  \n",
      "52           0.001129            0.001129         0.001129  \n",
      "53           0.001659            0.001659         0.001659  \n",
      "54           0.001111            0.001111         0.001111  \n",
      "55           0.001272            0.001272         0.001272  \n",
      "56           0.001562            0.001562         0.001562  \n",
      "\n",
      "[57 rows x 114 columns]\n",
      "_______________________________________________________\n",
      "modelo:  knn\n",
      "{'kneighborsclassifier__n_neighbors': [1], 'kneighborsclassifier__metric': ['cosine']}\n",
      "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n"
     ]
    }
   ],
   "source": [
    "results = pd.DataFrame()\n",
    "for name, model, parameter in models:\n",
    "    print('modelo: ', name)\n",
    "    print(parameter)\n",
    "    \n",
    "    tfidf=TfidfVectorizer(sublinear_tf=True, use_idf=True)\n",
    "    pipe_model = make_pipeline(tfidf, model)\n",
    "    gs = GridSearchCV(estimator=pipe_model, param_grid=parameter, scoring=scoring, refit='accuracy', cv = skf, return_train_score=True, verbose=5, n_jobs=4)\n",
    "    gs.fit(X, y)\n",
    "    print(\"Best Estimator: \\n{}\\n\".format(gs.best_estimator_))\n",
    "    print(\"Best Parameters: \\n{}\\n\".format(gs.best_params_))\n",
    "    print(\"Best Test Score: \\n{}\\n\".format(gs.best_score_))\n",
    "    d = gs.cv_results_\n",
    "    d['metodo'] = name\n",
    "    df = pd.DataFrame.from_dict(d)\n",
    "    results = results.append(df, ignore_index=True)\n",
    "    print(results)\n",
    "    \n",
    "    results.to_csv('resultados.csv', sep=';', decimal=',')\n",
    "    print('_______________________________________________________')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
